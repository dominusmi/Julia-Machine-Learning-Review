{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "In general boosting is an ensemble method which aims to combine\n",
    "the output of many *weak* learners in order to produce a single \n",
    "*strong* learner in relation to some objective function. In this\n",
    "context a *weak* learner will be only slightly correlated with a\n",
    "true classification i.e marginally better than random guessing.\n",
    "(NB boosting can be extended to regression). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "The AdaBoost algorithm solves a two class problem with \n",
    "features $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$ and outputs $y_{i}\\in\\{-1,1\\}$, for $i=1,2,\\ldots,n$. We consider a \n",
    "weak classifier $G(x)\\}$ and an error rate\n",
    "$$\n",
    "E(G,\\boldsymbol{x},y) = \\frac{\\sum_{i=1}^{N}\\bigl(1-\\delta_{y_{i},G(\\boldsymbol{x}_{i})}\\bigr)}{N}\n",
    "$$\n",
    "\n",
    "The algorithm sequentially applies this classifier to a modified\n",
    "version of the data $\\boldsymbol{x}$. This produces the sequence \n",
    "$\\{G_{m}(\\boldsymbol{x})\\}_{m=1}^{M}$ of classifiers. The data \n",
    "modification, at step $k$, is a weighting applied to each instance \n",
    "$(\\boldsymbol{x}_{i},y_{i})$ depending on wether the previous \n",
    "classifier, $G_{k-1}(x)$, correctly or incorrectly classified the \n",
    "instance. If classification was correct the weight decreases, and if \n",
    "incorrect the weight increases, with the initial weighting $\\frac{1}{N}$ for all the instances. Additionally a weight of\n",
    "$\\alpha_{k}$ is computed bases on the error \n",
    "$E(G_{k},\\boldsymbol{x},y)$ which determines the contribution of \n",
    "$G_{k}$ to the final classifier.\n",
    "\n",
    "In detail the *discrete* algorithm is as follows\n",
    "\n",
    "1. Set weights to $w_{i}=\\frac{1}{N}$ $\\forall$ $i$. \n",
    "2. For $m=1$ to $M$.\n",
    "    2. Fit $G_{m}$ to training data with weightings $w_{i}$.\n",
    "    2. Compute the weighted error \n",
    "    $$\n",
    "    E_{m} = \\frac{\\sum_{i=1}^{N}w_{i}(1-\\delta_{y_{i},G_{m}(\\boldsymbol{x}_{i})})}{\\sum_{i=1}^{N}w_{i}}\n",
    "    $$\n",
    "    2. Compute the final weight \n",
    "    $$\n",
    "        \\alpha_{i} = \\log\\frac{1-E_{m}}{E_{m}}\n",
    "    $$\n",
    "    2. Update the weight for the instances\n",
    "    $$\n",
    "        w_{i} \\leftarrow w_{i}e^{\\alpha_{m}(1-\\delta_{y_{i},G_{m}(\\boldsymbol{x}_{i})})}, i=1,2,\\ldots,N\n",
    "    $$\n",
    "3. Output the final classifier\n",
    "    $$\n",
    "        G(x) = \\text{sgn}\\biggl(\\sum_{m=1}^{M}\\alpha_{m}G_{m}(x)\\biggr)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost for decision trees\n",
    "\n",
    "In the decision tree context an example of a weak classifier is\n",
    "the decision stump, which includes one rule at the root node\n",
    "and two resultant leaf nodes. Using a decision stump may \n",
    "yield error rates of $\\sim 45\\%$ but applying AdaBoost (with \n",
    "$G$ the decision stump) can improve this to $\\sim 6\\%$ after \n",
    "400 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia Implementations\n",
    "\n",
    "### Libraries\n",
    "    \n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl (Decision Stumps)\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 10.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting as an additive expansion\n",
    "\n",
    "Boosting can be thought of as fitting an additive expansion \n",
    "$$\n",
    "    f(x) = \\sum_{m=1}^{M}\\beta_{m}b(x:\\gamma_{m})\n",
    "$$\n",
    "\n",
    "Where the $\\beta_{m}, m=1,2,\\ldots,M$ are expansion coefficients \n",
    "and $b(x:\\gamma)\\in\\mathbb{R}$ are *basis* functions. For AdaBoost\n",
    "the basis functions are the classifiers $G_{m}$. In this context a \n",
    "forward stagewise additive modeling algorithm may be constructed \n",
    "to produce more accurate approximations of $f$, this involves\n",
    "an iterative optimisation\n",
    "$$\n",
    "    (\\beta_{m},\\gamma_{m}) = \\text{argmin}_{\\beta,\\gamma}\\sum_{i=1}^{N}L((y_{i},f_{m-1}(x_{i})+\\beta b(x_{i}:\\gamma))\n",
    "$$\n",
    "\n",
    "such that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f_{m}(x) &= f_{m-1}(x)+\\beta_{m}b(x:\\gamma_{m})\\\\\n",
    "    f_{0}(x) &= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $L$ is some loss function. In this context AdaBoost is defined\n",
    "by using the loss\n",
    "$$\n",
    "    L(y,f(x)) = e^{-yf(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient boosting is a boosting algorithm which allows for the use \n",
    "of an arbirary (but differentiable) loss function. The intuition\n",
    "is to think of a boosting algorithm as an optimisation procedure in\n",
    "the form of stagewise additive modelling, but unlike the AdaBoost\n",
    "example above *weaknesses* are identified by negative gradients not by \n",
    "high-weight data points.\n",
    "\n",
    "\n",
    "The genral algorithm for an arbitrary differentiable loss,\n",
    "$L$, with parameters $\\gamma$ is as follows\n",
    "\n",
    "1. Set \n",
    "$$\n",
    "    f_{0}(x)=\\text{argmin}_{\\gamma}\\sum_{i=1}^{N}L(y_{i},\\gamma)\n",
    "$$\n",
    "2. For $m=1$ to $M$\n",
    "    2. For $i=1,2,\\ldots,N$ compute\n",
    "    $$\n",
    "        r_{im} = -\\biggl[\\frac{\\partial L(y_{i},f(x_{i}))}{\\partial f(x_{i})}\\biggr]_{f=f_{m-1}}\n",
    "    $$\n",
    "    2. Fit learner, $h_{m}(x)$ to the training set $\\{(x_{i},r_{im})\\}_{i=1}^{N}$\n",
    "    2. Compute\n",
    "    $$\n",
    "        \\gamma_{m} = \\text{argmin}_{\\gamma}\\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+\\gamma h_{m}(x))\n",
    "    $$\n",
    "    2. Update the model\n",
    "    $$\n",
    "        f_{m}(x) = f_{m-1}(x) +\\gamma_{m}h_{m}(x_{i})\n",
    "    $$\n",
    "3. Output the final model\n",
    "$$\n",
    "    \\hat{f}(x) = f_{M}(x)\n",
    "$$\n",
    "\n",
    "## Gradient Boosted CART\n",
    "\n",
    "If a tree model is used (CART is often gradient boosted), in step\n",
    "B tree models are fitted to the targets producing terminal regions\n",
    "$R_{jm}, j=1,2,\\ldots,J_{m}$, then C becomes\n",
    "$$\n",
    "    \\gamma_{jm} = \\text{argmin}_{\\gamma}\\sum_{x_{i}\\in R_{jm}}L(y_{i},f_{m-1}(x_{i})+\\gamma)\n",
    "$$\n",
    "\n",
    "and finally D becomes \n",
    "$$\n",
    "        f_{m}(x) = f_{m-1}(x) +\\sum_{j=1}^{J_{m}}\\gamma_{jm}\\boldsymbol{1}_{R_{jm}}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia Implementations\n",
    "\n",
    "### Libraries\n",
    "    \n",
    "- https://github.com/dmlc/XGBoost.jl\n",
    "- https://github.com/svs14/GradientBoost.jl\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 10.2, 10.4, 10.9, and 10.10)\n",
    "\n",
    "[2] Friedman, Jerome H, Greedy function approximation: a gradient boosting machine, Annals of statistics 2001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
