{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning frameworks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief description and comparison of two deep learning frameworks, Knet and Flux, which are implemented in Julia programming language. \n",
    "\n",
    "Knet supports GPU operation and automatic differentiation using dynamic computational graphs for models defined in plain Julia. The foundation of Knet is a package called AutoGrad which is an automatic differentiation package for Julia. This package follows the popular Python Autograd package. Flux is a framework in which the programmers can write normal Julia code and then by using Flux, the code can be run on tensorflow or other graph based deep learning framework as a backend.\n",
    "\n",
    "Knet is somehow equivalent to Tensorflow and Flux is like Keras in Python. This means that using Flux is easer than using Knet and they are less things that the programmers is dealing with, when using Flux. On the other hand, Knet is a frame work which is implemented completely in Julia, and it does not rely on any other framework in backend that can have a positive impact on the time required for training.\n",
    "\n",
    "One of interesting feature of Flux, that makes it easy to form different models, is defining layers with different shapes and different activation functions. This can be easily done by using Dense function, and then by using Chain, a list of layers can be defined and in order to stack them up and to arrange the shape of the model which is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Optimisers **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:60%\">\n",
    "  <tr >\n",
    "    <th style=\"text-align: left;\"><b>Optimiser</b></th>\n",
    "    <th style=\"text-align: left;\"><b>Knet</b></th>\n",
    "    <th style=\"text-align: left;\"><b>Flux</b></th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Stochastic gradient descent</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">Yes</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Momentum</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">Yes</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Nesterov's momentum</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">Yes</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Adagrad</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">No</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Adadelta</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">No</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Rmsprop</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">No</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Adam</td>\n",
    "    <td style=\"text-align: left;\">Yes</td>\n",
    "    <td style=\"text-align: left;\">Yes</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align: left;\">Nadam</td>\n",
    "    <td style=\"text-align: left;\">No</td>\n",
    "    <td style=\"text-align: left;\">No</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet\n",
    "using Flux, Flux.Data.MNIST\n",
    "using Flux.Tracker\n",
    "using Flux: onehotbatch, argmax, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Housing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data\",\"housing.jl\"))\n",
    "x,y = housing();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367.9544143509537\n",
      "242.50852102455386\n",
      "163.46664060246212\n",
      "113.09234753743877\n",
      "80.91832375406112\n",
      "60.33689670453079\n",
      "47.148583748042704\n",
      "38.68009965771645\n",
      "33.227860638866304\n",
      "29.705222335336902\n"
     ]
    }
   ],
   "source": [
    "predict(w,x) = w[1]*x .+ w[2]\n",
    "loss(w,x,y) = mean(abs2,y-predict(w,x))\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "function train(w, data; lr=.1)\n",
    "    for (x,y) in data\n",
    "        dw = lossgradient(w, x, y)\n",
    "    for i in 1:length(w)\n",
    "        w[i] -= lr * dw[i]\n",
    "    end    \n",
    "    end\n",
    "    return w\n",
    "end\n",
    "\n",
    "w = Any[ 0.1*randn(1,13), 0.0 ]\n",
    "for i=1:10\n",
    "    train(w, [(x,y)]);\n",
    "    println(loss(w,x,y)); \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Flux **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param(624.648)\n",
      "param(367.281)\n",
      "param(241.562)\n",
      "param(163.271)\n",
      "param(113.356)\n",
      "param(81.4281)\n",
      "param(60.9682)\n",
      "param(47.8293)\n",
      "param(39.3691)\n",
      "param(33.9022)\n"
     ]
    }
   ],
   "source": [
    "W = param(rand(1, 13))\n",
    "b = param(rand(1))\n",
    "predict(x) = W*x .+ b\n",
    "loss(x, y) = mean((predict(x) .- y).^2)\n",
    "#loss(x, y) = mean(abs2,y-predict(x))\n",
    "\n",
    "#l = loss(x, y);\n",
    "#back!(l)\n",
    "\n",
    "function update()\n",
    "  η = 0.1 # Learning Rate\n",
    "  for p in (W, b)\n",
    "    p.data .-= η .* p.grad # Apply the update\n",
    "    p.grad .= 0            # Clear the gradient\n",
    "  end\n",
    "end\n",
    "\n",
    "for i=1:10\n",
    "    l = loss(x, y);\n",
    "    back!(l)\n",
    "    update()\n",
    "    println(l)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (MNIST Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "dtrn = minibatch(xtrn, ytrn, 100)\n",
    "dtst = minibatch(xtst, ytst, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :trn, 0.11666666666666667, :tst, 0.1221)\n",
      "(:epoch, 1, :trn, 0.8997, :tst, 0.9033)\n",
      "(:epoch, 2, :trn, 0.9072333333333333, :tst, 0.9096)\n",
      "(:epoch, 3, :trn, 0.9111166666666667, :tst, 0.9117)\n",
      "(:epoch, 4, :trn, 0.9133833333333333, :tst, 0.9129)\n",
      "(:epoch, 5, :trn, 0.9149833333333334, :tst, 0.9142)\n",
      "(:epoch, 6, :trn, 0.9160833333333334, :tst, 0.9147)\n",
      "(:epoch, 7, :trn, 0.9170166666666667, :tst, 0.9144)\n",
      "(:epoch, 8, :trn, 0.9178333333333333, :tst, 0.9148)\n",
      "(:epoch, 9, :trn, 0.9184666666666667, :tst, 0.9152)\n",
      "(:epoch, 10, :trn, 0.9193166666666667, :tst, 0.916)\n"
     ]
    }
   ],
   "source": [
    "predict(w,x) = w[1]*mat(x) .+ w[2]\n",
    "loss(w,x,ygold) = nll(predict(w,x), ygold)\n",
    "lossgradient = grad(loss)\n",
    "\n",
    "w = Any[ 0.1f0*randn(Float32,10,784), zeros(Float32,10,1) ]\n",
    "println((:epoch, 0, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "for epoch=1:10\n",
    "    train(w, dtrn; lr=0.5)\n",
    "    println((:epoch, epoch, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Flux **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = MNIST.images()\n",
    "X = hcat(float.(reshape.(imgs, :))...)\n",
    "labels = MNIST.labels()\n",
    "Y = onehotbatch(labels, 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(X, Y) = param(2.27113)\n",
      "loss(X, Y) = param(1.46039)\n",
      "loss(X, Y) = param(1.1093)\n",
      "loss(X, Y) = param(0.928734)\n",
      "loss(X, Y) = param(0.82016)\n",
      "loss(X, Y) = param(0.742456)\n",
      "loss(X, Y) = param(0.69136)\n",
      "loss(X, Y) = param(0.655337)\n",
      "loss(X, Y) = param(0.624003)\n",
      "loss(X, Y) = param(0.598561)\n",
      "loss(X, Y) = param(0.57742)\n",
      "loss(X, Y) = param(0.55952)\n",
      "loss(X, Y) = param(0.544128)\n",
      "loss(X, Y) = param(0.530721)\n",
      "loss(X, Y) = param(0.518914)\n",
      "loss(X, Y) = param(0.508417)\n",
      "loss(X, Y) = param(0.499009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8854"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(Dense(784, 10), softmax)\n",
    "\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "accuracy1(x, y) = mean(argmax(m(x)) .== argmax(y))\n",
    "\n",
    "dataset = repeated((X, Y), 200)\n",
    "evalcb() = @show(loss(X, Y))\n",
    "opt = SGD(params(m))\n",
    "\n",
    "Flux.train!(loss, dataset, opt, cb = throttle(evalcb, 10))\n",
    "\n",
    "accuracy1(X, Y)\n",
    "\n",
    "# Test set accuracy\n",
    "tX = hcat(float.(reshape.(MNIST.images(:test), :))...)\n",
    "tY = onehotbatch(MNIST.labels(:test), 0:9)\n",
    "\n",
    "accuracy1(tX, tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muli-Layer Perceptron (MNIST Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Knet **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 0, :trn, 0.08423333333333333, :tst, 0.0838)\n",
      "(:epoch, 1, :trn, 0.9283166666666667, :tst, 0.9283)\n",
      "(:epoch, 2, :trn, 0.9486333333333333, :tst, 0.9453)\n",
      "(:epoch, 3, :trn, 0.9605166666666667, :tst, 0.9558)\n",
      "(:epoch, 4, :trn, 0.9671833333333333, :tst, 0.9618)\n",
      "(:epoch, 5, :trn, 0.9726666666666667, :tst, 0.9655)\n",
      "(:epoch, 6, :trn, 0.9773333333333334, :tst, 0.9675)\n",
      "(:epoch, 7, :trn, 0.97965, :tst, 0.9678)\n",
      "(:epoch, 8, :trn, 0.9813833333333334, :tst, 0.9691)\n",
      "(:epoch, 9, :trn, 0.9831666666666666, :tst, 0.9695)\n",
      "(:epoch, 10, :trn, 0.98445, :tst, 0.9702)\n"
     ]
    }
   ],
   "source": [
    "function predict(w,x)\n",
    "    x = mat(x)\n",
    "    for i=1:2:length(w)-2\n",
    "        x = relu.(w[i]*x .+ w[i+1])\n",
    "    end\n",
    "    return w[end-1]*x .+ w[end]\n",
    "end\n",
    "\n",
    "w = Any[ 0.1f0*randn(Float32,64,784), zeros(Float32,64,1),\n",
    "         0.1f0*randn(Float32,10,64),  zeros(Float32,10,1) ]\n",
    "\n",
    "function train(model, data, optim)\n",
    "    for (x,y) in data\n",
    "        grads = lossgradient(model,x,y)\n",
    "        update!(model, grads, optim)\n",
    "    end\n",
    "end\n",
    "\n",
    "o = optimizers(w, Adam)\n",
    "println((:epoch, 0, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "for epoch=1:10\n",
    "    train(w, dtrn, o)\n",
    "    println((:epoch, epoch, :trn, accuracy(w,dtrn,predict), :tst, accuracy(w,dtst,predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Flux **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(X, Y) = param(2.28075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Base.repeated is deprecated, use Base.Iterators"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(X, Y) = param(1.85265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".repeated instead.\n",
      "  likely near In[78]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(X, Y) = param(1.44539)\n",
      "loss(X, Y) = param(1.12403)\n",
      "loss(X, Y) = param(0.91184)\n",
      "loss(X, Y) = param(0.776611)\n",
      "loss(X, Y) = param(0.688019)\n",
      "loss(X, Y) = param(0.62645)\n",
      "loss(X, Y) = param(0.581399)\n",
      "loss(X, Y) = param(0.546992)\n",
      "loss(X, Y) = param(0.519825)\n",
      "loss(X, Y) = param(0.500026)\n",
      "loss(X, Y) = param(0.483293)\n",
      "loss(X, Y) = param(0.468935)\n",
      "loss(X, Y) = param(0.456461)\n",
      "loss(X, Y) = param(0.445507)\n",
      "loss(X, Y) = param(0.43579)\n",
      "loss(X, Y) = param(0.427099)\n",
      "loss(X, Y) = param(0.419269)\n",
      "loss(X, Y) = param(0.412163)\n",
      "loss(X, Y) = param(0.404905)\n",
      "loss(X, Y) = param(0.398315)\n",
      "loss(X, Y) = param(0.39229)\n",
      "loss(X, Y) = param(0.386753)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9006"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "  Dense(784, 32, relu),\n",
    "  Dense(32, 10),\n",
    "  softmax)\n",
    "\n",
    "opt = ADAM(params(m))\n",
    "\n",
    "Flux.train!(loss, dataset, opt, cb = throttle(evalcb, 10))\n",
    "\n",
    "accuracy1(X, Y)\n",
    "\n",
    "# Test set accuracy\n",
    "tX = hcat(float.(reshape.(MNIST.images(:test), :))...)\n",
    "tY = onehotbatch(MNIST.labels(:test), 0:9)\n",
    "\n",
    "accuracy1(tX, tY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
