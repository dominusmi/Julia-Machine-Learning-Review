{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatsBase: predict\n",
    "import Base: getindex\n",
    "using SparseRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable Task \n",
    "    task_type::String\n",
    "    target::Int\n",
    "    features::Array{Int}\n",
    "end\n",
    "\n",
    "function Task(;task_type=\"regression\", target=nothing, data=nothing)\n",
    "    if target == nothing || data == nothing\n",
    "        throw(\"Requires target and data to be set\")\n",
    "    end\n",
    "    \n",
    "    features = size(data,2)\n",
    "    features = deleteat!( collect(1:features), target)\n",
    "    \n",
    "    Task(task_type, target, features)\n",
    "end\n",
    "\n",
    "immutable Learner\n",
    "    name::String\n",
    "    parameters::Union{Void,Dict{String, Float64}}\n",
    "    Learner(learner::String) = new(learner, Dict())\n",
    "    Learner(learner::String, parameters::Dict{Any}) = new(learner, parameters)\n",
    "end\n",
    "\n",
    "immutable Resampling\n",
    "    method::String\n",
    "    iterations::Int\n",
    "end\n",
    "\n",
    "abstract type Parameter end\n",
    "\n",
    "\n",
    "immutable DiscreteParameter <: Parameter \n",
    "    name::String\n",
    "    values::Array{Any}\n",
    "end\n",
    "\n",
    "immutable ContinuousParameter <: Parameter\n",
    "    name::String\n",
    "    lower::Real\n",
    "    upper::Real\n",
    "    transform::Function\n",
    "    ContinuousParameter(;name=nothing, lower=nothing, upper=nothing, transform=nothing) = new(name, lower, upper, transform)\n",
    "end\n",
    "\n",
    "\n",
    "immutable ParametersSet\n",
    "   parameters::Array{Parameter}\n",
    "end\n",
    "\n",
    "getindex(p::ParametersSet, i::Int64) = p.parameters[i]\n",
    "\n",
    "immutable MLRModel{T}\n",
    "    model::T\n",
    "    parameters\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnᵧ (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### ABSTRACT FUNCTIONS ####\n",
    "\n",
    "function MLRModel(learner::Learner, task::Task, data)\n",
    "    if learner.name == \"ridge\"\n",
    "        makeRidge(learner, task, data)\n",
    "    end\n",
    "end    \n",
    "\n",
    "function learnᵧ(learner::Learner, task::Task, data)\n",
    "    modelᵧ = MLRModel(learner, task, data)\n",
    "    learnᵧ!(modelᵧ, learner=learner, task=task, data=data)\n",
    "    modelᵧ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnᵧ! (generic function with 2 methods)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### MODEL WRAPPERS ####\n",
    "using SparseRegression\n",
    "\n",
    "function get_λ(parameters, data)\n",
    "    if get(parameters, \"λ\", false) !== false\n",
    "        lambda = fill(0.0, size(data,2))\n",
    "    elseif typeof(parameters[\"λ\"]) <: Real\n",
    "        lambda = fill(0.0, size(data,2))\n",
    "    elseif typeof(parameters[\"λ\"]) <: Vector{Float64}\n",
    "        lambda = copy(parameters[\"λ\"])\n",
    "    end\n",
    "    lambda\n",
    "end\n",
    "\n",
    "function makeRidge(learner::Learner, task::Task, data)\n",
    "    if isempty(learner.parameters)\n",
    "        model = SModel(data[:, task.features], data[:, task.target])\n",
    "    else\n",
    "        parameters = []\n",
    "        push!(parameters, get_λ(learner.parameters, data))\n",
    "        model = SModel(data[:, task.features], data[:, task.target], L2DistLoss(), L2Penalty(), parameters...)\n",
    "    end\n",
    "    MLRModel(model, copy(learner.parameters))\n",
    "end\n",
    "\n",
    "function predictᵧ(modelᵧ::MLRModel{<:SModel}; data=data, task=task)\n",
    "    predict(modelᵧ.model, data[:, task.features])\n",
    "end\n",
    "\n",
    "function learnᵧ!(modelᵧ::MLRModel{<:SModel}; learner=nothing::Learner, data=nothing::Matrix{Real}, task=nothing::Task)\n",
    "    learn!(modelᵧ.model)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tune (generic function with 1 method)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_parameters!(array, range) \n",
    "    for i in 1:length(array)\n",
    "       if array[i] > range[i] \n",
    "            array[i+1] += 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function parameters_dictionary(ps::ParametersSet, array)\n",
    "    dict = Dict()\n",
    "    for i in 1:length(array)\n",
    "       dict[ps[i].name] = ps[i].transform( array[i] )\n",
    "    end\n",
    "    dict\n",
    "end\n",
    "\n",
    "function tune(;learner=nothing::Learner, task=nothing::Task, data=nothing::Matrix{Real}, parameters_set=nothing::ParametersSet)\n",
    "    n_parameters = length(parameters_set.parameters)\n",
    "    \n",
    "    parameters_array = zeros(n_parameters)\n",
    "    parameters_range = zeros(n_parameters)\n",
    "    \n",
    "    total_parameters = 0\n",
    "    \n",
    "    for i in 1:n_parameters\n",
    "        lower = parameters_set[i].lower\n",
    "        upper = parameters_set[i].upper\n",
    "        parameters_array[i] = lower\n",
    "        parameters_range[i] = upper\n",
    "        total_parameters += abs(upper-lower)\n",
    "    end\n",
    "    \n",
    "    for i in 1:total_parameters\n",
    "        update_parameters!(parameters_array, parameters_range) \n",
    "        pd = parameters_dictionary(parameters_set, parameters_array)\n",
    "        lrn = Learner(learner.name, pd)\n",
    "        modelᵧ = learnᵧ(lrn, task, data)\n",
    "#         println(\"Done model: $(modelᵧ)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeData (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function FakeData(N,d)\n",
    "    n_obs = 100\n",
    "    x = randn((n_obs,d))\n",
    "    y = 2*x+1.+randn(n_obs,d)*0.1\n",
    "    \n",
    "    hcat(x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model: MLRModel{SparseRegression.SModel{LossFunctions.LPDistLoss{2},PenaltyFunctions.L2Penalty,Array{Float64,2},Array{Float64,1},Void}}(█ SModel\n",
      "  > β        : [2.01226 -2.03728 1.00766]\n",
      "  > λ factor : [0.0 0.0 0.0 0.0]\n",
      "  > Loss     : L2DistLoss\n",
      "  > Penalty  : L2Penalty\n",
      "  > Data\n",
      "    - x : 100×3 Array{Float64,2}\n",
      "    - y : 100-element Array{Float64,1}\n",
      "    - w : Void, Dict(\"λ\"=>0.0001))\n",
      "Done model: MLRModel{SparseRegression.SModel{LossFunctions.LPDistLoss{2},PenaltyFunctions.L2Penalty,Array{Float64,2},Array{Float64,1},Void}}(█ SModel\n",
      "  > β        : [2.01226 -2.03728 1.00766]\n",
      "  > λ factor : [0.0 0.0 0.0 0.0]\n",
      "  > Loss     : L2DistLoss\n",
      "  > Penalty  : L2Penalty\n",
      "  > Data\n",
      "    - x : 100×3 Array{Float64,2}\n",
      "    - y : 100-element Array{Float64,1}\n",
      "    - w : Void, Dict(\"λ\"=>0.0001))\n",
      "Done model: MLRModel{SparseRegression.SModel{LossFunctions.LPDistLoss{2},PenaltyFunctions.L2Penalty,Array{Float64,2},Array{Float64,1},Void}}(█ SModel\n",
      "  > β        : [2.01226 -2.03728 1.00766]\n",
      "  > λ factor : [0.0 0.0 0.0 0.0]\n",
      "  > Loss     : L2DistLoss\n",
      "  > Penalty  : L2Penalty\n",
      "  > Data\n",
      "    - x : 100×3 Array{Float64,2}\n",
      "    - y : 100-element Array{Float64,1}\n",
      "    - w : Void, Dict(\"λ\"=>0.0001))\n",
      "Done model: MLRModel{SparseRegression.SModel{LossFunctions.LPDistLoss{2},PenaltyFunctions.L2Penalty,Array{Float64,2},Array{Float64,1},Void}}(█ SModel\n",
      "  > β        : [2.01226 -2.03728 1.00766]\n",
      "  > λ factor : [0.0 0.0 0.0 0.0]\n",
      "  > Loss     : L2DistLoss\n",
      "  > Penalty  : L2Penalty\n",
      "  > Data\n",
      "    - x : 100×3 Array{Float64,2}\n",
      "    - y : 100-element Array{Float64,1}\n",
      "    - w : Void, Dict(\"λ\"=>0.0001))\n",
      "Done model: MLRModel{SparseRegression.SModel{LossFunctions.LPDistLoss{2},PenaltyFunctions.L2Penalty,Array{Float64,2},Array{Float64,1},Void}}(█ SModel\n",
      "  > β        : [2.01226 -2.03728 1.00766]\n",
      "  > λ factor : [0.0 0.0 0.0 0.0]\n",
      "  > Loss     : L2DistLoss\n",
      "  > Penalty  : L2Penalty\n",
      "  > Data\n",
      "    - x : 100×3 Array{Float64,2}\n",
      "    - y : 100-element Array{Float64,1}\n",
      "    - w : Void, Dict(\"λ\"=>0.0001))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "ps = ParametersSet([\n",
    "    ContinuousParameter(\n",
    "            name = \"λ\",\n",
    "            lower = -4,\n",
    "            upper = 1,\n",
    "            transform = x->10^x\n",
    "        )\n",
    "])\n",
    "\n",
    "data = FakeData(1000,2)\n",
    "\n",
    "task = Task(task_type=\"regression\", target=3, data=data)\n",
    "lrn = Learner(\"ridge\")\n",
    "\n",
    "tune(learner=lrn, task=task, data=data, parameters_set=ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSweep finished\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20-element Array{Float64,1}:\n",
       " -1.14304  \n",
       " -1.70316  \n",
       "  1.51433  \n",
       "  1.38185  \n",
       "  0.398081 \n",
       "  0.488288 \n",
       " -0.62988  \n",
       "  0.0231981\n",
       "  3.00679  \n",
       "  0.335098 \n",
       "  1.3501   \n",
       " -1.06243  \n",
       "  1.02961  \n",
       " -0.259217 \n",
       "  2.96875  \n",
       "  0.153039 \n",
       " -0.300593 \n",
       "  0.905237 \n",
       " -0.331963 \n",
       "  1.15747  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = FakeData(1000,2)\n",
    "\n",
    "task = Task(task_type=\"regression\", target=3, data=data)\n",
    "lrn  = Learner(\"ridge\")\n",
    "\n",
    "train = 1:80\n",
    "test  = 81:100\n",
    "\n",
    "\n",
    "modelᵧ = learnᵧ(lrn, task, data[train,:])\n",
    "predictᵧ(modelᵧ, data=data[test,:], task=task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
