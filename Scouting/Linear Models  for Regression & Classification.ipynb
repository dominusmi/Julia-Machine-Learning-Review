{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Regression\n",
    "\n",
    "let $\\textbf{X} \\in \\rm I\\!R^{Nxp}$ be a set of input data with targets $\\textbf{y} \\in \\rm I\\!R^{N}$, where $N$ is the number of datapoints, and $p$ their dimensionality, meaning this data comes as a matrix where rows are datapoints, and columns are features.\n",
    "\n",
    "Let $\\textbf{x}$ be a generic variable representing any row (or datapoint) of this matrix, and $x_i$ be the $j^{th}$ column (or feature) of this datapoint.\n",
    "\n",
    "Regression models can be used to try and and find the coefficients $\\beta_{0,..,p}$ such that \n",
    "\n",
    "$$ \\beta_0 + \\sum_{j=1}^{p}\\beta_{j} x_j $$\n",
    "\n",
    "minimises the Residual Sum of Squares (RSS) over all the datapoints, possibly subject to some contraint.\n",
    "\n",
    "In this notebook, only univariate regression is discussed.\n",
    "\n",
    "**Note on transformations:** although the input datapoints $x$ are often referred to as those coming directly from the dataset, this is not at all a requirement, in fact it can sometimes be extremely useful to apply some function to these before submitting them to the algorithms, which is one simple way of using a linear model to solve non-linear problems. \n",
    "Some simple transformations are:\n",
    "-  the polynomial expansion $\\textbf{x} = (x_1, x_1^2, x_1^2,.., x_1^p)$\n",
    "-  cross combinations $\\textbf{x} = (x_1, x_2, x_3, ..., x_1 x_2, x_1 x_3, ... )$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square (OLS)\n",
    "_also known as Linear Least Square (LLS/LLSq)_\n",
    "\n",
    "\n",
    "OLS is the simplest of these methods. We try to find the linear function $f(x;\\beta)$ which minimises:\n",
    "\n",
    "$$ RSS(\\beta) = \\sum_{i=1}^{N} (y_i - f(\\textbf{x};\\beta))^2 $$\n",
    "\n",
    "where $ f(\\textbf{x};\\beta) = \\beta_0 + \\sum_{j=1}^{p}\\beta_{j} x_j $\n",
    "\n",
    "Note that this can be written in matrix form by adding one column of $1s$ to the $\\textbf{x}$ vector, such that $\\textbf{x} = (1, x_1, x_2, .., x_p)$, then\n",
    "\n",
    "$$ f(x;\\beta) = \\textbf{x} \\beta $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ RSS(\\beta) =  \\sum_{i=1}^{N} (y_i -\\beta \\textbf{x})^2 $$\n",
    "$$ \\quad \\Leftrightarrow (\\textbf{y} - \\textbf{X} \\beta )^T (\\textbf{y} - \\textbf{X} \\beta ) $$\n",
    "in matrix form.\n",
    "\n",
    "The vector $\\beta$ which minimises is then \n",
    "\n",
    "$$ \\hat{\\beta} = ( \\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T\\textbf{y} $$\n",
    "\n",
    "\n",
    "### Julia implementations\n",
    "\n",
    "**Libraries:**\n",
    "-  SparseRegression.jl [_git_](https://github.com/joshday/SparseRegression.jl) [_notebook_](Sparse%20Regression.ipynb)\n",
    "-  MultivariateStats.jl [_git_](https://github.com/juliaStats/MultivariateStats.jl) [_notebook_](MultivariateStats.ipynb) \n",
    "-  GLM.jl\n",
    "\n",
    "_Note: MultivariateStats.jl allows for multi-variate regression_\n",
    "\n",
    "** Simple code: **\n",
    "```julia\n",
    "# Simple (but suboptimal) implementation\n",
    "function OLSEstimator(x,y)\n",
    "    β = inv(x'*x)*(x'*y)\n",
    "    return β\n",
    "end\n",
    "```\n",
    "\n",
    "<img src=\"resources/LR_comparison_ols.png\" width=600></img>\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "[1] The Elements of Statistical Learning (_Ch 4.1_)                                                  \n",
    "[2] https://juliaeconomics.com/2014/06/15/introductory-example-ordinary-least-squares/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "\n",
    "### Introduction\n",
    "One of the main problems with OLS is the prediction accuracy and the high variance. The prediction accuracy can sometimes be solved through subset selection, more or less assuming that many of the features given have different importance for the estimation, and therefore should not be given the same importance as is done in in _OLS_. \n",
    "\n",
    "_Ridge regression_ tries to solve this by adding a penalty on the coefficients, forcing the lowering of some of them so that others can increase.\n",
    "\n",
    "This also solves an interpretation problem, whereas we can visualise how important a coefficient is compared to the others by their magnitude.\n",
    "\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "_Ridge regression_ formulation is \n",
    "\n",
    "$$ \\beta_{ridge} = \\mathop{\\mathrm{arg\\,min}}_\\beta \\big \\{ \\sum_{i=1}^{N}(y_i - \\beta_0 \\sum_{j=1}^{p} x_{ij} \\beta_j )^2 + \\lambda \\sum_{j=1}^{p}\\beta_{j}^2 \\big \\} $$\n",
    "\n",
    "where $\\lambda$ is the regularizer term. A low $\\lambda$ gives low regularization, with $\\lambda=0$ giving _OLS_ back, while large $\\lambda$ means strong regularization, forcing some coefficients closer to 0.\n",
    "\n",
    "The problem can be reformulated as a contraint problem\n",
    "\n",
    "$$ \\beta_{rigde} = \\mathop{\\mathrm{arg\\,min}}_\\beta \\textit{OLS} \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\le t $$\n",
    "\n",
    "The vector $\\beta$ which minimises the RSS is then\n",
    "\n",
    "$$ \\hat{\\beta}_{ridge} = ( \\textbf{X}^T \\textbf{X} +\\lambda \\text{I} )^{-1} \\textbf{X}^T\\textbf{y} $$\n",
    "\n",
    "**Note:** $\\lambda$ is often also called $\\alpha$, such as in scikit-learn models  \n",
    "\n",
    "\n",
    "### Julia implementations\n",
    "\n",
    "**Libraries:**\n",
    "-  SparseRegression.jl [_git_](https://github.com/joshday/SparseRegression.jl) [_notebook_](Sparse%20Regression.ipynb)\n",
    "-  MultivariateStats.jl [_git_](https://github.com/juliaStats/MultivariateStats.jl) [_notebook_](MultivariateStats.ipynb)\n",
    "-  GLM.jl\n",
    "\n",
    "_Note: MultivariateStats.jl allows for multi-variate regression_\n",
    "\n",
    "\n",
    "** Simple code: **\n",
    "```julia\n",
    "# Simple (but suboptimal) implementation\n",
    "function RidgeEstimator(x,y,λ,n)\n",
    "    β = inv(x'*x + λ*eye(n,n) )*(x'*y)\n",
    "    return β\n",
    "end\n",
    "```\n",
    "\n",
    "<img src=\"resources/LR_comparison_ridge.png\" width=600></img>\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "[1] The Elements of Statistical Learning  _(Ch 4.4)_                                            \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Lasso is a stronger regularization method, which uses the $L1$ norm for the regulatrization (absolute value) instead of the $L2$ in ridge. This makes for stronger regularization, likely to set some coefficients to 0 entirely.\n",
    "\n",
    "It can therefore be used as a continuous subset selection, continuous in the sense that $\\lambda$ is a continuous variable, and subset selection since coefficients will actually be set to 0 and therefore the features associated will not be considered at all.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "_Lasso regression_ formulation is \n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_{lasso} = \\mathop{\\mathrm{arg\\,min}}_\\beta \\big \\{ \\sum_{i=1}^{N}(y_i - \\beta_0 \\sum_{j=1}^{p} x_{ij} \\beta_j )^2 + \\lambda \\sum_{j=1}^{p} | \\: \\beta_{j} \\:  | \\big \\} \n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the regularizer term. A low $\\lambda$ gives low regularization, with $\\lambda=0$ giving _OLS_ back, while large $\\lambda$ means strong regularization.\n",
    "\n",
    "The problem can be reformulated as a contraint problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_{lasso} = \\mathop{\\mathrm{arg\\,min}}_\\beta \\textit{OLS} \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} | \\: \\beta_j \\: | \\le t \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "There is no matrix formulation for the optimal values of $\\beta$, although efficient algorithms are available, which makes finding lasso solution as fast as ridge, for example using a lasso modified _Least Angle Regression_ (LAR) algorithm.\n",
    "\n",
    "**Note:** $\\lambda$ is often also called $\\alpha$, such as in scikit-learn models  \n",
    "\n",
    "**Practical Notice:** for lasso to work properly, data should be normalised.\n",
    "\n",
    "#### Example of $\\beta$ coefficients evolution with \n",
    "\n",
    "![lol](resources/lasso_coefficients.png)\n",
    "\n",
    "### Julia implementations\n",
    "\n",
    "**Libraries:**\n",
    "-  SparseRegression.jl [_git_](https://github.com/joshday/SparseRegression.jl) [_notebook_](Sparse%20Regression.ipynb)\n",
    "-  GLM.jl\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "[1] The Elements of Statistical Learning  _(Ch 4.4)_                                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Linear Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty straightforward to use the above discussed methods for classification problem using multivariate regression.\n",
    "Let $n$ be the number of classes of a classification problem, $\\textbf{X} \\in \\rm I\\!R^{Nxp}$  be the datapoints and $y \\in \\rm I\\!R^{N}$ the target classes. Then $\\textbf{y}=(y_1, y_2, ..., y_N)$ where $y_i \\in (1,..,n)$. These targets can be transformed into a binary $N \\times n$ matrix, where each row only has one non-zero element, corresponding to the class column.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    0       & 1 & 0 & \\dots & 0 \\\\\n",
    "    1       & 0 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    0       & 0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "would mean the target class of the first observation is 1, the second target is 0, and the $N^{th}$ target is n\n",
    "\n",
    "Then it is possible to train a multivariate regression model to try and \"regress\" these. If the model is linear, such as _OLS_, then the regression will give continuous values for every target class, which can then be interpreted as _certainty_ (can further be intepreted as \"probabilities\" if transformed through a function such as _softmax_ )\n",
    "\n",
    "### Julia implementations\n",
    "\n",
    "There are no explicit implementations that directly allow for classification, but it is quite simple to setup using regression methods.\n",
    "An example can be found in the [MultivariateStats notebook](MultivariateStats.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
