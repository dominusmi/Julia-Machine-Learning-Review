{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "In general boosting is an ensemble method which aims to combine\n",
    "the output of many *weak* learners in order to produce a single \n",
    "*strong* learner in relation to some objective function. In this\n",
    "context a *weak* learner will be only slightly correlated with a\n",
    "true classification i.e marginally better than random guessing.\n",
    "(NB boosting can be extended to regression). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "The AdaBoost algorithm solves a two class problem with \n",
    "features $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$ and outputs $y_{i}\\in\\{-1,1\\}$, for $i=1,2,\\ldots,n$. We consider a \n",
    "weak classifier $G(x)\\}$ and an error rate\n",
    "$$\n",
    "E(G,\\boldsymbol{x},y) = \\frac{\\sum_{i=1}^{N}\\bigl(1-\\delta_{y_{i},G(\\boldsymbol{x}_{i})}\\bigr)}{N}\n",
    "$$\n",
    "\n",
    "The algorithm sequentially applies this classifier to a modified\n",
    "version of the data $\\boldsymbol{x}$. This produces the sequence \n",
    "$\\{G_{m}(\\boldsymbol{x})\\}_{m=1}^{M}$ of classifiers. The data \n",
    "modification, at step $k$, is a weighting applied to each instance \n",
    "$(\\boldsymbol{x}_{i},y_{i})$ depending on wether the previous \n",
    "classifier, $G_{k-1}(x)$, correctly or incorrectly classified the \n",
    "instance. If classification was correct the weight decreases, and if \n",
    "incorrect the weight increases, with the initial weighting $\\frac{1}{N}$ for all the instances. Additionally a weight of\n",
    "$\\alpha_{k}$ is computed bases on the error \n",
    "$E(G_{k},\\boldsymbol{x},y)$ which determines the contribution of \n",
    "$G_{k}$ to the final classifier.\n",
    "\n",
    "In detail the algorithm is as follows\n",
    "\n",
    "1. Set weights to $w_{i}=\\frac{1}{N}$ $\\forall$ $i$. \n",
    "2. For $m=1$ to $M$.\n",
    "    2. Fit $G_{m}$ to training data with weightings $w_{i}$.\n",
    "    2. Compute the weighted error \n",
    "    $$\n",
    "    E_{m} = \\frac{\\sum_{i=1}^{N}w_{i}(1-\\delta_{y_{i},G_{m}(\\boldsymbol{x}_{i})})}{\\sum_{i=1}^{N}w_{i}}\n",
    "    $$\n",
    "    2. Compute the final weight \n",
    "    $$\n",
    "        \\alpha_{i} = \\log\\frac{1-E_{m}}{E_{m}}\n",
    "    $$\n",
    "    2. Update the weight for the instances\n",
    "    $$\n",
    "        w_{i} \\leftarrow w_{i}e^{\\alpha_{m}(1-\\delta_{y_{i},G_{m}(\\boldsymbol{x}_{i})})}, i=1,2,\\ldots,N\n",
    "    $$\n",
    "3. Output the final classifier\n",
    "    $$\n",
    "        G(x) = \\text{sgn}\\biggl(\\sum_{m=1}^{M}\\alpha_{m}G_{m}(x)\\biggr)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost for decision trees\n",
    "\n",
    "In the decision tree context an example of a weak classifier is\n",
    "the decision stump, which includes one rule at the root node\n",
    "and two resultant leaf nodes. Using a decision stump may \n",
    "yield error rates of $\\sim 45\\%$ but applying AdaBoost (with \n",
    "$G$ the decision stump) can improve this to $\\sim 6\\%$ after \n",
    "400 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia Implementations\n",
    "\n",
    "### Libraries\n",
    "    \n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl (Decision Stumps)\n",
    "\n",
    "### References\n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 10.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
