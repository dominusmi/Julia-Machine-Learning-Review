{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseRegression.jl\n",
    "Git: https://github.com/joshday/SparseRegression.jl\n",
    "\n",
    "\n",
    "\n",
    "### TODO:\n",
    "\n",
    "-  Add more extensive benchmark\n",
    "-  Find out what $\\omega$ parameter is\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Sparse regression is a package to achieve high performance regression of linear models for large dataset where coefficients often turn out to be sparse.\n",
    "The main call follows the form SModel(x,y, args) where arguments include the loss, penalty, and the $\\lambda$ and $\\omega$ arguments.\n",
    "Prediction are done through *predict(X, model)* call\n",
    "\n",
    "The loss and penalty functions are based on the _LossFunctions_ and _PenaltyFunctions_ MLJulia core packages.\n",
    "\n",
    "Additionally, one can use learning strategies from the _LearningStategies_ package. This allows to set parameters that are purely learning based, such as optimizers, max iterations or max items. \n",
    "More on this in the documentation.\n",
    "\n",
    "This structure allows for one model to be used for the many linear models such as OLS, ridge, lasso etc. which all have the same underlying structure.\n",
    "\n",
    "Issues: Seems to lose performance quite strongly when dimensionality increases, see benchmark at the bottom\n",
    "\n",
    "---\n",
    "### Details\n",
    "\n",
    "| Test        | Results           \n",
    "| ------------- |:-------------:|\n",
    "| Package works | yes |\n",
    "| Deprecations warnings      | No      |\n",
    "| Compatible with JuliaDB | If targets transformed into array |\n",
    "| Contains documentation | yes, but not great |\n",
    "| Simplicity | good |\n",
    "\n",
    "\n",
    "---\n",
    "### Usage\n",
    "\n",
    "\n",
    "**SModel(x, y, args...)**\n",
    "\n",
    "Arguments\n",
    "\n",
    "- loss::Loss = .5 * L2DistLoss()\n",
    "\n",
    "- penalty::Penalty = L2Penalty()\n",
    "\n",
    "- λ::Vector{Float64} = fill(size(x, 2), .1)\n",
    "\n",
    "- w::Union{Void, AbstractWeights} = nothing\n",
    "\n",
    "_Note: these are interpreted from the type by the function, not from keys_\n",
    "\n",
    "Using personalised learning strategies [(see more)](https://github.com/JuliaML/LearningStrategies.jl):\n",
    "\n",
    "```\n",
    "learn!(s, strategy(ProxGrad(s), MaxIter(25), TimeLimit(.5)))\n",
    "```\n",
    "\n",
    "**Important:** Although the library supposedly accepts integer arrays, it doesn't seem to actually work in practice\n",
    "\n",
    "---\n",
    "\n",
    "### Differences with Python's scikit-learn\n",
    "\n",
    "-  Scikit-learn offers the *fit_intercept* parameter, which here is not a choice.\n",
    "-  $\\alpha$ parameter, $\\lambda$ in this package, can only be given as a scalar in scikit, while this package allows the user to pass a vector, allowing different regularizations on features.\n",
    "-  Scikit offers to normalize the data, here it is assumed the user will have done any preprocessing before using the model\n",
    "-  Scikit offers to precompute the Gram matrix, which can speed up calculations\n",
    "-  Scikit offers an argument named _positive_, which forces the coefficients to be positive\n",
    "-  Scikit allows to give a seed for which coefficient to update, if the argument _selection_ is set to \"random\"\n",
    "-  In scikit, all these different linear models use different functions, while this package groups them by simply allowing the user to select the loss and penalty, allowing for greater flexibility\n",
    "-  Through the use of _LearningStrategies_, this package allows a lot of flexibility on the user side as to how to learn. More on these strategies can be found [here](https://github.com/JuliaML/LearningStrategies.jl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Incorporation of different models\n",
    "\n",
    "**Ridge** \n",
    "`model = SModel(X_train,y_train, L2DistLoss(), L2Penalty());`\n",
    "\n",
    "**Lasso**\n",
    "`model = SModel(X_train,y_train, L2DistLoss(), L1Penalty());`\n",
    "\n",
    "**Elastic-Net**\n",
    "`model = SModel(X_train,y_train, L2DistLoss(), ElasticNetPenalty(α = 0.5));`\n",
    "\n",
    "_Note: these penalties have default parameters, for more information, see the [git hub repository](https://github.com/JuliaML/PenaltyFunctions.jl)_\n",
    "\n",
    "___\n",
    "\n",
    "### Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseRegression;\n",
    "include(\"load_titanic.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mMaxIter(100) finished\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "█ SModel\n",
       "  > β        : [2.52083e74 -4.01126e74 … -2.42626e75 -1.07446e73]\n",
       "  > λ factor : [0.1 0.1 … 0.1 0.1]\n",
       "  > Loss     : L2DistLoss\n",
       "  > Penalty  : L1Penalty\n",
       "  > Data\n",
       "    - x : 634×8 Array{Float64,2}\n",
       "    - y : 634-element Array{Int64,1}\n",
       "    - w : Void"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example using lasso regression\n",
    "model = SModel(X_train,y_train, L2DistLoss(), L1Penalty());\n",
    "learn!(model);\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting new data\n",
    "predict(model, X_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (Very) Simple benchmark vs python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Only lasso regression is tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_regression(n_points::Int64, n_dims::Int64)\n",
    "    x = randn(n_points, n_dims);\n",
    "    y = x * linspace(-1, 1, n_dims) + randn(n_points);\n",
    "    s = SModel(x, y, L2DistLoss(), L1Penalty(), fill(1.0, size(x, 2)));\n",
    "\n",
    "    tic();\n",
    "    learn!(s);\n",
    "    time = toc();\n",
    "   \n",
    "    return time\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IJulia.set_verbose(false)\n",
    "\n",
    "n_points = 10_000\n",
    "n_dims = [10, 100, 1000, 5000]\n",
    "\n",
    "avg_times = []\n",
    "\n",
    "for n_dim in n_dims\n",
    "    times = []\n",
    "    for i in 1:5\n",
    "        time = compute_regression(n_points, n_dim);\n",
    "        \n",
    "        push!(times, time);\n",
    "    end\n",
    "    avg_times = mean(times);\n",
    "    println(\"Average time $(avg_times) for $(n_dim) dimensions\")\n",
    "end\n",
    "\n",
    "IJulia.set_verbose(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "| Dimensions    | Julia | Python    \n",
    "| ------------- |:-----:|:-----:|\n",
    "| 10 | 0.0028s | 0.023s |\n",
    "| 100 | 0.0078s | 0.19s |\n",
    "| 1000 | 0.61s | 2.3s|\n",
    "| 5000 | 4.59s | 17s|\n",
    "\n",
    "The code for the python's results can be found in *python_scripts.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 634\n",
    "n_dims = 8\n",
    "x = randn(n_points, n_dims);\n",
    "y = x * linspace(-1, 1, n_dims) + randn(n_points);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "█ SModel\n",
       "  > β        : [0.0 0.0 … 0.0 0.0]\n",
       "  > λ factor : [0.1 0.1 … 0.1 0.1]\n",
       "  > Loss     : 0.5 * (L2DistLoss)\n",
       "  > Penalty  : L2Penalty\n",
       "  > Data\n",
       "    - x : 634×8 Array{Float64,2}\n",
       "    - y : 634-element Array{Float64,1}\n",
       "    - w : Void"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SModel(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mMaxIter(100) finished\n",
      "\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"mismatch in dimension 2 (expected 0 got 1)\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"mismatch in dimension 2 (expected 0 got 1)\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m_cs\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Int64, ::Bool, ::Int64, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./abstractarray.jl:1193\u001b[22m\u001b[22m",
      " [2] \u001b[1m_cshp\u001b[22m\u001b[22m at \u001b[1m./abstractarray.jl:1181\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1m_cshp\u001b[22m\u001b[22m at \u001b[1m./abstractarray.jl:1190\u001b[22m\u001b[22m [inlined]",
      " [4] \u001b[1mcat_shape\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Bool}, ::Tuple{Int64,Int64}, ::Tuple{Int64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./abstractarray.jl:1170\u001b[22m\u001b[22m (repeats 2 times)",
      " [5] \u001b[1mcat_t\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{T} where T, ::Type{T} where T, ::Array{Any,2}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./abstractarray.jl:1203\u001b[22m\u001b[22m",
      " [6] \u001b[1mvcat\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,2}, ::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./sparse/sparsevector.jl:990\u001b[22m\u001b[22m",
      " [7] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[124]:13\u001b[22m\u001b[22m [inlined]",
      " [8] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [9] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "using MLPreprocessing\n",
    "X_train, y_train, X_test, y_test = load();\n",
    "y_train = convert(Array{Float64}, y_train)\n",
    "\n",
    "scaler = fit(StandardScaler, X_train, obsdim=2);\n",
    "X_train = MLPreprocessing.transform(X_train, scaler);\n",
    "\n",
    "βs = Matrix(0,8)\n",
    "\n",
    "for λ in 0.01:0.001:0.02\n",
    "    model = SModel(X_train, y_train, L2DistLoss(), L1Penalty(), fill(0.015, size(x, 2)) );\n",
    "    learn!(model);\n",
    "    βs = [βs; model.β]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0×8 Array{Any,2}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "βs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
