{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnlineStats.jl\n",
    "\n",
    "_git:_ https://github.com/joshday/OnlineStats.jl                          \n",
    "_documentation:_ http://joshday.github.io/OnlineStats.jl/stable/index.html\n",
    "\n",
    "## Summary\n",
    "\n",
    "Online Stats provide multipel analysis algorithms together with a framework to apply them to online data, this means that instead of requiring all the data to be loaded and analysed at once, one can feed it over time and update the current stat. \n",
    "(Note: we follow the documnetation's convention which uses _stat_ to name any statistics/models that the library can handle)\n",
    "\n",
    "This is an extremely interesting feature since it not only allows to work with data coming through a live feed, it also allows the user to divide the input up into smaller parts if it is too large to be handled at once, for instance for memory usage. Additionally, the library allows merging of its _stats_, making parallelization extremely simple, since the data can be divided into subsets, each fitted separately, and then merged at the end.\n",
    "\n",
    "\n",
    "The library includes stats ranging from mean and variance, to statistical learning methods such as SVM and multivariate analysis such as PCA. A full list of _stats_ it supports can be found [here](http://joshday.github.io/OnlineStats.jl/stable/stats_and_models.html#Statistics-and-Models-1)\n",
    "\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Basic\n",
    "The package works mostly using only two functions\n",
    "\n",
    "```julia\n",
    "    s = Series( Stats1(), Stats2(), .. (, x, Weight() ) )\n",
    "```\n",
    "\n",
    "Here we instantiate a Series, using $x$ as initial input (optional), any number of _stats_ from the models list listed above, and a _weight_ function, which determines how the weight of observation varies as more observations are added.\n",
    "\n",
    "\n",
    "Next we are fitting the new inputs y to the already instantiated series s. Can additionally tell the fitting to use columns or rows for observations (defaults to row).\n",
    "\n",
    "```julia\n",
    "    fit!(s, y (,major) )\n",
    "```\n",
    "where _major_\n",
    "should be either _Cols()_ or _Rows()_, and specifies which of _row_ or _column major_ conventions for matrix is being used.\n",
    "\n",
    "Finally, we can get the value of a stat at any point by invoking the _value_ function:\n",
    "\n",
    "```julia\n",
    "    value(s)\n",
    "```\n",
    "\n",
    "### Weights\n",
    "\n",
    "The choice of weights is quite important in an online method since every merge and update will use it to attribute the amount of influence of the observation. OnlineStats already contains a large library of weights, and additionally allows for custom weight functions to be passed. More information can be found [here](http://joshday.github.io/OnlineStats.jl/stable/weights.html)\n",
    "\n",
    "Weights are passed as arugment to the _Series_ function, for example using mean and variance as _stats_:\n",
    "\n",
    "```julia\n",
    "    Series(x, WeightFunction(), Mean(), Variance()) \n",
    "```\n",
    "\n",
    "The following plot shows the different weight functions and how they change with the number of data points added.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/8075494/29486708-a52b9de6-84ba-11e7-86c5-debfc5a80cca.png\" width=\"500\"/>\n",
    "\n",
    "### Merging\n",
    "\n",
    "If two series follow the same models, they can be merged. Additionally, options can be given as two the merging method\n",
    "\n",
    "```julia  \n",
    "    # Default\n",
    "    merge!(s1, s2, :append)  # equivalent to fit!(s1, y1); fit!(s1, y2);\n",
    "\n",
    "    # Using weighted average\n",
    "    merge!(s1, s2, :weighted)\n",
    "\n",
    "    # Treat s2 as a singleton\n",
    "    merge!(s1, s2, :singleton)\n",
    "\n",
    "    # Provide ratio of influence s2 should have\n",
    "    merge!(s1, s2, .5)    \n",
    "```\n",
    "\n",
    "\n",
    "### Parallelization\n",
    "\n",
    "Parallelization is then simply done by fitting the _series_ made of the same _stats_ but given different subsets of the dataset. Note that not all _stats_ will give the exact result when merged compared to if they were fitted as one. The models that do allow for exact merging are of the subtype _ExactStat_.\n",
    "\n",
    "```julia\n",
    "    y1 = randn(10_000)\n",
    "    y2 = randn(10_000)\n",
    "    y3 = randn(10_000)\n",
    "\n",
    "    s1 = Series(Mean(), Variance(), Hist(50))\n",
    "    s2 = Series(Mean(), Variance(), Hist(50))\n",
    "    s3 = Series(Mean(), Variance(), Hist(50))\n",
    "\n",
    "    fit!(s1, y1)\n",
    "    fit!(s2, y2)\n",
    "    fit!(s3, y3)\n",
    "\n",
    "    merge!(s1, s2)  # merge information from s2 into s1\n",
    "    merge!(s1, s3)  # merge information from s3 into s1\n",
    "\n",
    "```\n",
    "\n",
    "### Statistical Learning Methods\n",
    "\n",
    "The objective function that OnlineStats tries to minimize is \n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i}^{n} f_i(\\beta) + \\sum_{j=1}^{p} \\lambda_{p} g(\\beta_j) $$\n",
    "\n",
    "Choosing the right penalties and losses can then produce a multitude of models, including Ridge, Lasso, SVMs, Logistic.\n",
    "\n",
    "```julia\n",
    "    ### Doc ###\n",
    "\n",
    "    StatLearn(p::Int, args)\n",
    "    args:\n",
    "        loss = .5 * L2DistLoss() # any Loss from LossFunctions.jl\n",
    "        penalty = L2Penalty() # any Penalty (which has a prox method) from PenaltyFunctions.jl.\n",
    "        λ = fill(.1, p) # a Vector of element-wise regularization parameters\n",
    "        updater = SGD() # SGD, ADAGRAD, ADAM, ADAMAX, MSPI\n",
    "\n",
    "    ### Usage ### \n",
    "\n",
    "    o = StatLearn(10, .5 * L2DistLoss(), L1Penalty(), fill(.1, 10), SGD())\n",
    "    s = Series(o)\n",
    "    fit!(s, x, y)\n",
    "    coef(o)\n",
    "    predict(o, x)\n",
    "```\n",
    "\n",
    "### Other methods\n",
    "\n",
    "\n",
    "Linear regression [_(documentation)_](http://joshday.github.io/OnlineStats.jl/stable/api.html#OnlineStats.LinRegBuilder)\n",
    "```julia \n",
    "    lr = LinRegBuilder(p) # p = number of dimensions\n",
    "    Series((x,y), lr) # Will do simple OLS\n",
    "    value(lr)\n",
    "\n",
    "    # Can provide additional arguments with coef function\n",
    "    Series(x,lr)\n",
    "    coef(lr, λ, y=[7], x=[1,2,3]) # Will use columns 1,2,3 to predict column 7 of x\n",
    "    coef(lr, .5) # Make Ridge by specifying λ (can be float of vector of p dimensions)\n",
    "    value(lr)\n",
    "```\n",
    "\n",
    "Calculate covariance matrix and PCA\n",
    "```julia\n",
    "    y = randn(1000, 5)\n",
    "    o = CovMatrix(5)\n",
    "    Series(y, o)\n",
    "\n",
    "    # PCA & tranformation\n",
    "    d_out = 2\n",
    "    corr = cor(o)\n",
    "    evals, evecs = eig(corr)\n",
    "    pc = sortperm(evals, rev=true)[1:d_out]\n",
    "    T = y * evecs[:,pc];\n",
    "```\n",
    "\n",
    "___\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Weights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OnlineStats\n",
    "using Plots\n",
    "plotly()\n",
    "\n",
    "steps = 0:0.1:6*pi\n",
    "sins = Float64[]\n",
    "means = zeros(Float64, 0, 6)\n",
    "\n",
    "labels = [\"EqualWeight\", \"ExponentialWeight\", \"Bounded(EqualWeight(),.2)\", \"LearningRate(.6)\", \n",
    "        \"HarmonicWeight(.1)\", \"McclainWeight(.1)\"]\n",
    "\n",
    "s_equal = Series(EqualWeight(), Mean() )\n",
    "s_exp = Series(ExponentialWeight(), Mean() )\n",
    "s_bounded = Series(Bounded(EqualWeight(),.2), Mean())\n",
    "s_lr = Series( LearningRate(.6), Mean())\n",
    "s_harmonic = Series(HarmonicWeight(.1), Mean())\n",
    "s_mcclain = Series(McclainWeight(.1), Mean())\n",
    "\n",
    "for (i, x) in enumerate(steps)\n",
    "    append!(sins, sin(x))\n",
    "    fit!(s_equal, sins)\n",
    "    fit!(s_exp, sins)\n",
    "    fit!(s_bounded, sins)\n",
    "    fit!(s_lr, sins)\n",
    "    fit!(s_harmonic, sins)\n",
    "    fit!(s_mcclain, sins)\n",
    "        \n",
    "    means = vcat(means, [value(s_equal)[1] value(s_exp)[1] value(s_bounded)[1] value(s_lr)[1] value(s_harmonic)[1] value(s_mcclain)[1]])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "![Weights comparison](resources/weights_comparison.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Learning (Ridge as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(x) = sqrt(mean(x.^2.))\n",
    "f(x) = 2*x[1] - 3*x[2]+0.1*x[3] + x[4] # Ground truth\n",
    "\n",
    "# Prepare data, with σ=0.1 normal noise\n",
    "x_train = randn(1000, 4)\n",
    "y_train = [f(x_train[i,:])+0.1*randn() for i=1:size(x_train,1) ]\n",
    "\n",
    "x_test = randn(50, 4)\n",
    "y_test = [f(x_test[i,:])+0.1*randn() for i=1:size(x_test,1) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model: Ridge with 4 input dimensions, using λ=0.1 for all features, and SGD optimiser\n",
    "o = StatLearn(4, L2Penalty(), L2DistLoss(), fill(0.1, 4), SGD())\n",
    "\n",
    "# Make it into a series and fit it\n",
    "s = Series(o);\n",
    "fit!(s, (x_train, y_train));\n",
    "\n",
    "# Found coefficients\n",
    "coef(o);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the testing set: 0.19317829923435953\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "score = rmse( y_test - predict(o, x_test) )\n",
    "println(\"Score on the testing set: $score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Linear Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(x) = sqrt(mean(x.^2.))\n",
    "f(x) = 2*x[1] - 3*x[2]+0.1*x[3] + x[4] - 7*x[5] + 2.4*x[6] # Ground truth\n",
    "\n",
    "# Prepare data, with σ=0.1 normal noise\n",
    "x_train = randn(30_000_000, 6)\n",
    "noise = 0.1*randn(30_000_000)\n",
    "y_train = [f(x_train[i,:])+noise[i] for i=1:size(x_train,1) ]\n",
    "\n",
    "x_test = randn(50, 6)\n",
    "y_test = [f(x_test[i,:])+0.1*randn() for i=1:size(x_test,1) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model: Ridge with 6 input dimensions, using λ=0.1 for all features, and SGD optimiser\n",
    "o = StatLearn(6, L2Penalty(), L2DistLoss(), fill(0.1, 6), SGD())\n",
    "\n",
    "# Make it into a series and fit it\n",
    "s = Series(o);\n",
    "\n",
    "### Single thread ###\n",
    "tic()\n",
    "fit!(s, (x_train, y_train));\n",
    "_end = toq();\n",
    "\n",
    "println(\"Single thread took $(_end) seconds\")\n",
    "\n",
    "### In parallel ###\n",
    "s1 = Series(StatLearn(6, L2Penalty(), L2DistLoss(), fill(0.1, 6), SGD()))\n",
    "s2 = Series(StatLearn(6, L2Penalty(), L2DistLoss(), fill(0.1, 6), SGD()))\n",
    "s3 = Series(StatLearn(6, L2Penalty(), L2DistLoss(), fill(0.1, 6), SGD()))\n",
    "\n",
    "tic()\n",
    "@spawn fit!(s1, (x_train[1:10_000_000,:], y_train[1:10_000_000]))\n",
    "@spawn fit!(s2, (x_train[10_000_001:20_000_000,:], y_train[10_000_001:20_000_000]))\n",
    "@spawn fit!(s3, (x_train[20_000_001:30_000_000,:], y_train[20_000_001:30_000_000]))\n",
    "\n",
    "merge!(s1, s2)  # merge information from s2 into s1\n",
    "merge!(s1, s3)  # merge information from s3 into s1\n",
    "_end = toq();\n",
    "\n",
    "\n",
    "println(\"Parallel took $(_end) seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Sanity check ###\n",
      "Coefficients comparison:\n",
      "\n",
      "Normal: 1.908 \tParallel: 1.909\n",
      "Normal: -2.856 \tParallel: -2.859\n",
      "Normal: 0.093 \tParallel: 0.09\n",
      "Normal: 0.953 \tParallel: 0.949\n",
      "Normal: -6.664 \tParallel: -6.669\n",
      "Normal: 2.282 \tParallel: 2.289\n"
     ]
    }
   ],
   "source": [
    "println(\"### Sanity check ###\\nCoefficients comparison:\\n\")\n",
    "n_coefs = coef(o)\n",
    "p_coefs = value(s1)[1]\n",
    "for i in 1:6\n",
    "    println(\"Normal: $(round(n_coefs[i],3)) \\tParallel: $(round(p_coefs[i],3))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.90809, -2.85603, 0.0931034, 0.952989, -6.66414, 2.28211],)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
