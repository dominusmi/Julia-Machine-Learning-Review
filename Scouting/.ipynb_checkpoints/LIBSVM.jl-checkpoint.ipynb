{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBSVM.jl\n",
    "\n",
    "https://github.com/mpastell/LIBSVM.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage LIBSVM is already installed\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"LIBSVM.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mCloning cache of ScikitLearn from https://github.com/cstjean/ScikitLearn.jl.git\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mInstalling ScikitLearn v0.3.0\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mBuilding Conda\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mBuilding SpecialFunctions\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mBuilding PyCall\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPyCall is using /usr/bin/python (Python 2.7.12) at /usr/bin/python, libpython = libpython2.7\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m/home/annika/JuliaProDir/JuliaPro-0.6.2.1/JuliaPro/pkgs-0.6.2.1/v0.6/PyCall/deps/deps.jl has been updated\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36m/home/annika/JuliaProDir/JuliaPro-0.6.2.1/JuliaPro/pkgs-0.6.2.1/v0.6/PyCall/deps/PYTHON has been updated\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage database updated\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"ScikitLearn.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "LIBSVM.jl is a package that wraps around the C library LIBSVM. All standard SVM models are usable with the LIBSVM API (SVC, Nu-SVC, SVR) and different kernel functions are provided. Some things that are missing are for example a preprocessing tool like svm-scale and cross validation. For the mathematical theory behind support vector machines and a comparision of this implementation with others please see https://github.com/dominusmi/warwick-rsg/blob/master/Scouting/SupportVectorMachinesTheory.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test        | Results       |    \n",
    "| ------------|:-------------:|\n",
    "| Package work| Yes |\n",
    "| Deprecations warnings      | No     |\n",
    "| Compatible with JuliaDB | Yes |\n",
    "| Contains documentation | Yes, as inline documentation. Type ?Functionname|\n",
    "| Simplicity | medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "To list the functions the inline documentation function is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1ms\u001b[22m\u001b[1mv\u001b[22m\u001b[1mm\u001b[22m\u001b[1mt\u001b[22m\u001b[1mr\u001b[22m\u001b[1ma\u001b[22m\u001b[1mi\u001b[22m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```julia\n",
       "svmtrain{T, U<:Real}(X::AbstractMatrix{U}, y::AbstractVector{T}=[];\n",
       "    svmtype::Type=SVC, kernel::Kernel.KERNEL=Kernel.RadialBasis, degree::Integer=3,\n",
       "    gamma::Float64=1.0/size(X, 1), coef0::Float64=0.0,\n",
       "    cost::Float64=1.0, nu::Float64=0.5, epsilon::Float64=0.1,\n",
       "    tolerance::Float64=0.001, shrinking::Bool=true,\n",
       "    probability::Bool=false, weights::Union{Dict{T, Float64}, Void}=nothing,\n",
       "    cachesize::Float64=200.0, verbose::Bool=false)\n",
       "```\n",
       "\n",
       "Train Support Vector Machine using LIBSVM using response vector `y` and training data `X`. The shape of `X` needs to be (nfeatures, nsamples). For one-class SVM use only `X`.\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `svmtype::Type=LIBSVM.SVC`: Type of SVM to train `SVC` (for C-SVM), `NuSVC`   `OneClassSVM`, `EpsilonSVR` or `NuSVR`. Defaults to `OneClassSVM` if   `y` is not used.\n",
       "  * `kernel::Kernels.KERNEL=Kernel.RadialBasis`: Model kernel `Linear`, `polynomial`,   `RadialBasis`, `Sigmoid` or `Precomputed`.\n",
       "  * `degree::Integer=3`: Kernel degree. Used for polynomial kernel\n",
       "  * `gamma::Float64=1.0/size(X, 1)` : γ for kernels\n",
       "  * `coef0::Float64=0.0`: parameter for sigmoid and polynomial kernel\n",
       "  * `cost::Float64=1.0`: cost parameter C of C-SVC, epsilon-SVR, and nu-SVR\n",
       "  * `nu::Float64=0.5`: parameter nu of nu-SVC, one-class SVM, and nu-SVR\n",
       "  * `epsilon::Float64=0.1`: epsilon in loss function of epsilon-SVR\n",
       "  * `tolerance::Float64=0.001`: tolerance of termination criterion\n",
       "  * `shrinking::Bool=true`: whether to use the shrinking heuristics\n",
       "  * `probability::Bool=false`: whether to train a SVC or SVR model for probability estimates\n",
       "  * `weights::Union{Dict{T, Float64}, Void}=nothing`: dictionary of class weights\n",
       "  * `cachesize::Float64=100.0`: cache memory size in MB\n",
       "  * `verbose::Bool=false`: print training output from LIBSVM if true\n",
       "\n",
       "Consult LIBSVM documentation for advice on the choise of correct parameters and model tuning.\n"
      ],
      "text/plain": [
       "```julia\n",
       "svmtrain{T, U<:Real}(X::AbstractMatrix{U}, y::AbstractVector{T}=[];\n",
       "    svmtype::Type=SVC, kernel::Kernel.KERNEL=Kernel.RadialBasis, degree::Integer=3,\n",
       "    gamma::Float64=1.0/size(X, 1), coef0::Float64=0.0,\n",
       "    cost::Float64=1.0, nu::Float64=0.5, epsilon::Float64=0.1,\n",
       "    tolerance::Float64=0.001, shrinking::Bool=true,\n",
       "    probability::Bool=false, weights::Union{Dict{T, Float64}, Void}=nothing,\n",
       "    cachesize::Float64=200.0, verbose::Bool=false)\n",
       "```\n",
       "\n",
       "Train Support Vector Machine using LIBSVM using response vector `y` and training data `X`. The shape of `X` needs to be (nfeatures, nsamples). For one-class SVM use only `X`.\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `svmtype::Type=LIBSVM.SVC`: Type of SVM to train `SVC` (for C-SVM), `NuSVC`   `OneClassSVM`, `EpsilonSVR` or `NuSVR`. Defaults to `OneClassSVM` if   `y` is not used.\n",
       "  * `kernel::Kernels.KERNEL=Kernel.RadialBasis`: Model kernel `Linear`, `polynomial`,   `RadialBasis`, `Sigmoid` or `Precomputed`.\n",
       "  * `degree::Integer=3`: Kernel degree. Used for polynomial kernel\n",
       "  * `gamma::Float64=1.0/size(X, 1)` : γ for kernels\n",
       "  * `coef0::Float64=0.0`: parameter for sigmoid and polynomial kernel\n",
       "  * `cost::Float64=1.0`: cost parameter C of C-SVC, epsilon-SVR, and nu-SVR\n",
       "  * `nu::Float64=0.5`: parameter nu of nu-SVC, one-class SVM, and nu-SVR\n",
       "  * `epsilon::Float64=0.1`: epsilon in loss function of epsilon-SVR\n",
       "  * `tolerance::Float64=0.001`: tolerance of termination criterion\n",
       "  * `shrinking::Bool=true`: whether to use the shrinking heuristics\n",
       "  * `probability::Bool=false`: whether to train a SVC or SVR model for probability estimates\n",
       "  * `weights::Union{Dict{T, Float64}, Void}=nothing`: dictionary of class weights\n",
       "  * `cachesize::Float64=100.0`: cache memory size in MB\n",
       "  * `verbose::Bool=false`: print training output from LIBSVM if true\n",
       "\n",
       "Consult LIBSVM documentation for advice on the choise of correct parameters and model tuning.\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?svmtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1ms\u001b[22m\u001b[1mv\u001b[22m\u001b[1mm\u001b[22m\u001b[1mp\u001b[22m\u001b[1mr\u001b[22m\u001b[1me\u001b[22m\u001b[1md\u001b[22m\u001b[1mi\u001b[22m\u001b[1mc\u001b[22m\u001b[1mt\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`svmpredict{T,U<:Real}(model::SVM{T}, X::AbstractMatrix{U})`\n",
       "\n",
       "Predict values using `model` based on data `X`. The shape of `X` needs to be (nsamples, nfeatures). The method returns tuple (predictions, decisionvalues).\n"
      ],
      "text/plain": [
       "`svmpredict{T,U<:Real}(model::SVM{T}, X::AbstractMatrix{U})`\n",
       "\n",
       "Predict values using `model` based on data `X`. The shape of `X` needs to be (nsamples, nfeatures). The method returns tuple (predictions, decisionvalues).\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?svmpredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most important function in this package. There are several others which are necessary for the interface with scikitlearn.jl. This interface seems to not be working though (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LIBSVM\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification (titianic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both JuliaDB and PyPlot export \"table\"; uses of it in module Main must be qualified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " / 1 files can be loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "include(\"load_titanic.jl\")\n",
    "train, train_targets, test, test_targets = load();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the first 80 instances of the dataset as test data, the remaining ones as training data. The data is grouped in two classes (\"survived\", \"not survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=transpose(train); #transpose the train array and the test array so the function can handle it\n",
    "test = transpose(test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run the classifier on the raw data as the package doesn't include any preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.25%\n"
     ]
    }
   ],
   "source": [
    "model = svmtrain(train,train_targets,probability=true)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relatively bad accucarcy here is probably due to the fact that we did no preprocessing. Training an SVC with a radial basis kernel in python on the same dataset gives a similar accuracy (using sklearn see https://github.com/dominusmi/warwick-rsg/blob/master/Scouting/Python%20Examples%20Benchmarking.ipynb). To improve classification results we want to preprocess the data such that it has zero mean and unit variance. This preprocessing tool is not built in yet, therefore we do it by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (train-mean(train))/std(train)\n",
    "test = (test-mean(test))/std(test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the fitting and predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "model = svmtrain(train,train_targets,kernel=Kernel.Linear)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy only improved slightly. In Python, using the preprocessing tools like for example Standard.Scaler() (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler), we can achieve and accuracy of around 80%. We next want to try some other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "#linear\n",
    "model = svmtrain(train,train_targets,kernel=Kernel.Linear)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.75%\n"
     ]
    }
   ],
   "source": [
    "#polynomial\n",
    "model = svmtrain(train,train_targets,kernel=Kernel.Polynomial)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid\n",
    "model = svmtrain(train,train_targets,kernel=Kernel.Sigmoid)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.75%\n"
     ]
    }
   ],
   "source": [
    "#Precomputed\n",
    "\n",
    "model = svmtrain(train,train_targets,kernel=Kernel.Precomputed)   #train the classifier (default: radial basis kernel)\n",
    "\n",
    "(predicted_labels, decision_values) = svmpredict(model, test);\n",
    "\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== test_targets))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification (iris dataset)\n",
    "\n",
    "This code example is taken from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "using RDatasets, LIBSVM\n",
    "\n",
    "# Load Fisher's classic iris data\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "# LIBSVM handles multi-class data automatically using a one-against-one strategy\n",
    "labels = convert(Vector, iris[:Species])\n",
    "\n",
    "# First dimension of input data is features; second is instances\n",
    "instances = convert(Array, iris[:, 1:4])'\n",
    "\n",
    "# Train SVM on half of the data using default parameters. See documentation\n",
    "# of svmtrain for options\n",
    "model = svmtrain(instances[:, 1:2:end], labels[1:2:end]);\n",
    "\n",
    "# Test model on the other half of the data.\n",
    "(predicted_labels, decision_values) = svmpredict(model, instances[:, 2:2:end]);\n",
    "\n",
    "# Compute accuracy\n",
    "@printf \"Accuracy: %.2f%%\\n\" mean((predicted_labels .== labels[2:2:end]))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikitlearn.jl interface\n",
    "\n",
    "This doesn't seem to be working very well. This is most likely a problem with scikitlearn.jl. See rewiev of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: fit! not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: fit! not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "using LIBSVM\n",
    "using ScikitLearn\n",
    "using RDatasets: dataset\n",
    "\n",
    "#Classification C-SVM\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "labels = convert(Vector, iris[:, :Species])\n",
    "instances = convert(Array, iris[:, 1:4])\n",
    "model = fit!(SVC(), instances[1:2:end, :], labels[1:2:end])\n",
    "yp = predict(model, instances[2:2:end, :])\n",
    "\n",
    "#epsilon-regression\n",
    "whiteside = RDatasets.dataset(\"MASS\", \"whiteside\")\n",
    "X = Array(whiteside[:Gas])\n",
    "y = Array(whiteside[:Temp])\n",
    "svrmod = fit!(EpsilonSVR(cost = 10., gamma = 1.), X, y)\n",
    "yp = predict(svrmod, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
