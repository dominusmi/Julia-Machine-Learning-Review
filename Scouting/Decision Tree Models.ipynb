{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Models\n",
    "\n",
    "A decision tree is a supervised learning method, suitable for \n",
    "classification and regression, which constructs an\n",
    "IF $\\rightarrow$ THEN ruleset (disjunction of conjuctions) inferred \n",
    "from the features of input data. \n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "|:-||:-|\n",
    "| Simple to interpret | Often overfit |\n",
    "| Low complexity, $O(log(n))$| Sensitive to data bias |\n",
    "| Handle numerical and categorical data | Unstable on small data variations|\n",
    "\n",
    "Notes\n",
    "\n",
    "- Decision trees algorithms are greedy, because searching all possible trees is insurmountable.\n",
    "- Many methods exits to reduce the disadvantages such as pruning and ensemble methods like random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART algorithm\n",
    "\n",
    "The CART (classification and regression trees) algorithm produces \n",
    "a classification or regression tree if the targets are categorical \n",
    "or numeric. All resulting trees are binary.\n",
    "\n",
    "In general the problem consists of $n$ training and target variables\n",
    "$\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$, $i=1,\\ldots,n$, $\\boldsymbol{y}\\in\\mathbb{R}^{l}$ respectively. Where the entries of $\\boldsymbol{y}$\n",
    "may be continuous values (for regression) or instances of c classes in\n",
    "$\\{0,1,\\ldots,c-1\\}$. \n",
    "\n",
    "The tree splits the data at node $m$, $D_{m}$, by an impurity measure \n",
    "$H$. That is for each \n",
    "canditate split $\\theta = (j,t_{m})$ the sets\n",
    "$$\n",
    "\\begin{align*}\n",
    "    D_{l}(\\theta) &= \\{(x,y)\\in D:x_{j} \\leq t_{m}\\}\\\\\n",
    "    D_{r}(\\theta) &= D \\backslash D_{l}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "are formed and the gain, $G$, is calculated, where \n",
    "$$\n",
    "G(D,\\theta) = \\frac{|D_{l}|}{|D_{m}|}H(D_{l}(\\theta))+\\frac{|D_{r}|}{|D_{m}|}H(D_{r}(\\theta))\n",
    "$$\n",
    "finally the best split, $\\Theta$, is found by\n",
    "$$\n",
    "\\begin{align*}\n",
    "        \\Theta &= \\text{ argmax}_{\\theta}G(D,\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "### Classification \n",
    "\n",
    "In classification $\\boldsymbol{y}$ has entries in $\\{0,1,\\ldots,c-1\\}$ \n",
    "we define the proportion of observations at node $m$ with class $k$ by\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{mk} = \\frac{1}{|D_{m}|}\\sum_{\\boldsymbol{x}_{i}\\in \n",
    "    D_{m}}\\delta_{k,\\boldsymbol{y}_{i}}\n",
    "\\end{align*}\n",
    "$$\n",
    "Then common impurity measures for classification include:\n",
    "\n",
    "Entropy $H(D_{m}) = -\\sum_{k}p_{mk}\\log_{b}(p_{mk})$.\n",
    "\n",
    "Gini index: $H(D_{m}) = \\sum_{k}p_mk(1-p_{mk})$.\n",
    "\n",
    "### Regression \n",
    "\n",
    "In regression $\\boldsymbol{y}$ has continuous entries and a measure\n",
    "such as the mean squared error is used for impurity, that is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    H(D_{m}) = \\frac{1}{|D_{m}|}\\sum_{\\boldsymbol{y}_{i}\\in D_{m}}(\\boldsymbol{y_{i}}-<\\boldsymbol{y_{i}}\\in D_{m}>)^{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $<x> = \\frac{1}{|x|}\\sum_{i=1}^{|x|}x_{i}$\n",
    "\n",
    "### Julia Implementations\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl\n",
    "- ScikitLearn.jl https://github.com/cstjean/ScikitLearn.jl\n",
    "\n",
    "### References\n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 9.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 and C4.5 Algorithms\n",
    "\n",
    "### ID3\n",
    "\n",
    "The Iterative Cishotomiser 3 (ID3) algorithm builds a classification \n",
    "tree using the information gain (from entropy) as a splitting \n",
    "measure. Tree are not necessarily binary.\n",
    "\n",
    "The process is similar to CART but at each node, $m$, of the tree we\n",
    "calculate the information gain, $IG$, for each attribute not\n",
    "already used in the tree. That is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    IG(D_{m},A) &= H(D_{m}) - \\sum_{i}\\frac{|D_{m,u_{i}}|}{|D_{m}|}H(D_{m,u_{i}}) \\\\\n",
    "    H(X) &= -\\sum_{i=1}^{C}p_{i}\\log_{2}p_{i}\n",
    "\\end{align*}\n",
    "$$\n",
    "Where $A$ is an attribute with values $u$, $D_{m}$ the data at \n",
    "node $m$, $D_{m,u_{i}}$ the subset of $D_{m}$ where $A=u_{i}$ and\n",
    "$p_{i}$ the proportion of elements of class $i$ in $X$. The data\n",
    "is then split along the attribute with the highest information gain\n",
    "(largest entropy reduction) forming $|u|$ branches at the node $m$.\n",
    "\n",
    "### C4.5\n",
    "\n",
    "C4.5 is a successor to ID3 the major improvement is to use the\n",
    "normalised information gain. At each node, $m$, the information gain\n",
    "is calculated as above but is normalised by the intrinsic information\n",
    "of the split, Isplit. Thats is\n",
    "$$\n",
    "\\begin{align*}\n",
    "Isplit(D_{m},A) &= -\\sum_{i}\\frac{|D_{m,u_{i}}|}{|D_{m}|}\\log_{2}\\frac{|D_{m,u_{i}}|}{|D_{m}|}\\\\\n",
    "IG_{norm}(D_{m},A) &= \\frac{IG(D_{m},A)}{Isplit(D_{m},A)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then the attribute with the highest normalised information gain is \n",
    "used for splitting. This has the impact of reducing the ID3 \n",
    "algorithm's bias to attributes with many values (highest branching \n",
    "factors in splits).\n",
    "\n",
    "Additionally C4.5 implements thresholding of continuous attributes. \n",
    "For each continuous attribute this creates a large list of \n",
    "pseudo-attributes representing all possible splits of the\n",
    "continous attribute.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993\n",
    "\n",
    "[2] Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81â€“106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A random forest is a modified bagging method which constructs a large \n",
    "set of *un-correlated* decision tree models and then averages over\n",
    "them. Random forests may also be used to rank the \n",
    "importance of features and inform feature selection.\n",
    "\n",
    "Assuming a forest size of $N$ and input data $D$, for each \n",
    "$b\\in\\{1,2,\\ldots,N\\}$. \n",
    "\n",
    "1. A boostrap sample $B\\subset D$ of size $n$ is drawn from the training data\n",
    "1. Grow a tree $T_{b}$ on $B$ by repeating the following until a minimum node size $n_{min}$ is reached \n",
    "    1. Select $m$ variables at random from $p$ variables\n",
    "    1. Pick the best split among the $m$ variables.\n",
    "    1. Split the node into two child nodes.\n",
    "\n",
    "When this is done for all $b$ output the forest $\\{T_{b}\\}_{b=1}^{N}$. \n",
    "\n",
    "\n",
    "For prediction of new data, $x$, in regression and classification we\n",
    "take\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{f}(x) &= \\sum_{b=1}^{N}T_{b}(x) \\\\\n",
    "    \\hat{\\text{C}}(x) &= \\text{ argmax}_{c}\\sum_{b=1}^{N}\\delta_{c,T_{b}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\hat{f}$ and $\\hat{C}$ are the estimated function and classification in respectively.\n",
    "\n",
    "### Out of Bag Sampling\n",
    "\n",
    "Out-of-bag (OOB) sampling refers to the way a random forest test \n",
    "predictions on the training set. For each observation \n",
    "$d_{i} = (x_{i},y_{i})$ the prediction of $z_{i}$ is obtained\n",
    "by averaging over all trees trained over boostrap samples where\n",
    "$z_{i}$ was absent. This is almost identical to the result of a \n",
    "$K$-fold cross validation, but completed in-line.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Decision trees naturally rank features during construction. That is\n",
    "at each split the increase in the splitting criterion (gain) is \n",
    "maximised, by recording this at each split variable importances can\n",
    "be generated.\n",
    "\n",
    "To get an accurate prediction of feature importances this concept is\n",
    "applied over random forests. Using OOB sampling a measure is \n",
    "constructed by, for each tree in the forest, recording the\n",
    "accuracy of prediction using the OOB samples, then the values\n",
    "of the $j$th feature are randomly permuted in the OOB samples and\n",
    "the resultant decrease in performance is recorded. Finally the\n",
    "decrease in performance after permuting the values of $j$ are averaged\n",
    "over all trees in the forest, this is the importance of $j$.\n",
    "\n",
    "#### Example Feature Importance Plot\n",
    "\n",
    "<img src=\"resources/titanic_import.png\">\n",
    "\n",
    "### Julia Implementations\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl\n",
    "- ScikitLearn.jl https://github.com/cstjean/ScikitLearn.jl\n",
    "\n",
    "### References \n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 15.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
