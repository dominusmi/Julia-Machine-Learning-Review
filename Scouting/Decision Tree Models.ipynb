{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Models\n",
    "\n",
    "A decision tree is a supervised learning method, suitable for \n",
    "classification and regression, which constructs an\n",
    "IF $\\rightarrow$ THEN ruleset (disjunction of conjuctions) inferred \n",
    "from the features of input data. \n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "|:-||:-|\n",
    "| Simple to interpret | Often overfit |\n",
    "| Low complexity, $O(log(n))$| Sensitive to data bias |\n",
    "| Handle numerical and categorical data | Unstable on small data variations|\n",
    "\n",
    "Notes\n",
    "\n",
    "- Decision trees algorithms are greedy, because searching all possible trees is insurmountable.\n",
    "- Many methods exits to reduce the disadvantages such as pruning and ensemble methods like random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART algorithm\n",
    "\n",
    "The CART (classification and regression trees) algorithm produces \n",
    "a classification or regression tree if the targets are categorical \n",
    "or numeric. All resulting trees are binary.\n",
    "\n",
    "In general the problem consists of $n$ training and target variables\n",
    "$\\boldsymbol{x}_{i}\\in\\mathbb{R}^{d}$, $i=1,\\ldots,n$, $\\boldsymbol{y}\\in\\mathbb{R}^{l}$ respectively. Where the entries of $\\boldsymbol{y}$\n",
    "may be continuous values (for regression) or instances of c classes in\n",
    "$\\{0,1,\\ldots,c-1\\}$. \n",
    "\n",
    "The tree splits the data at node $m$, $D_{m}$, by an impurity measure \n",
    "$H$. That is for each \n",
    "canditate split $\\theta = (j,t_{m})$ the sets\n",
    "\\begin{align*}\n",
    "    D_{l}(\\theta) &= \\{(x,y)\\in D:x_{j} \\leq t_{m}\\}\\\\\n",
    "    D_{r}(\\theta) &= D \\backslash D_{l}(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "are formed and the gain, $G$, is calculated, where \n",
    "\\begin{align*}\n",
    "    G(D,\\theta) &= \\frac{|D_{l}|}{|D_{m}|}H(D_{l}(\\theta))+\\frac{|D_{r}|}{|D_{m}|}H(D_{r}(\\theta))\n",
    "\\end{align*}\n",
    "\n",
    "finally the best split, $\\Theta$, is found by\n",
    "\n",
    "\\begin{align*}\n",
    "        \\Theta &= \\text{ argmax}_{\\theta}G(D,\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "### Classification \n",
    "\n",
    "In classification $\\boldsymbol{y}$ has entries in $\\{0,1,\\ldots,c-1\\}$ \n",
    "we define the proportion of observations at node $m$ with class $k$ by\n",
    "\\begin{align*}\n",
    "    p_{mk} = \\frac{1}{|D_{m}|}\\sum_{\\boldsymbol{x}_{i}\\in D_{m}}\\delta_{k,\\boldsymbol{y}_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "Then common impurity measures for classification include:\n",
    "\n",
    "Entropy $H(D_{m}) = -\\sum_{k}p_{mk}\\log_{b}(p_{mk})$.\n",
    "\n",
    "Gini index: $H(D_{m}) = \\sum_{k}p_mk(1-p_{mk})$.\n",
    "\n",
    "### Regression \n",
    "\n",
    "In regression $\\boldsymbol{y}$ has continuous entries and a measure\n",
    "such as the mean squared error is used for impurity, that is\n",
    "\n",
    "\\begin{align*}\n",
    "    H(D_{m}) = \\frac{1}{|D_{m}|}\\sum_{\\boldsymbol{y}_{i}\\in D_{m}}(\\boldsymbol{y_{i}}-<\\boldsymbol{y_{i}}\\in D_{m}>)^{2}\n",
    "\\end{align*}\n",
    "\n",
    "where $<x> = \\frac{1}{|x|}\\sum_{i=1}^{|x|}x_{i}$\n",
    "\n",
    "### Julia Implementations\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl\n",
    "- ScikitLearn.jl https://github.com/cstjean/ScikitLearn.jl\n",
    "\n",
    "### References\n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 9.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A random forest is a modified bagging method which constructs a large \n",
    "set of *un-correlated* decision tree models and then averages over\n",
    "them.\n",
    "\n",
    "Assuming a forest size of $N$ and input data $D$, for each \n",
    "$b\\in\\{1,2,\\ldots,N\\}$. \n",
    "\n",
    "1. A boostrap sample $B\\subset D$ of size $n$ is drawn from the training data\n",
    "1. Grow a tree $T_{b}$ on $B$ by repeating the following until a minimum node size $n_{min}$ is reached \n",
    "    1. Select $m$ variables at random from $p$ variables\n",
    "    1. Pick the best split among the $m$ variables.\n",
    "    1. Split the node into two child nodes.\n",
    "\n",
    "When this is done for all $b$ output the forest $\\{T_{b}\\}_{b=1}^{N}$. \n",
    "\n",
    "\n",
    "For prediction of new data in regression and classification $x$ we take\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{f}(x) &= \\sum_{b=1}^{N}T_{b}(x) \\\\\n",
    "    \\hat{\\text{C}}(x) &= \\text{ argmax}_{c}\\sum_{b=1}^{N}\\delta_{c,T_{b}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\hat{f}$ and $\\hat{C}$ are the estimated function and classification in respectively.\n",
    "\n",
    "### Julia Implementations\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "- DecisionTree.jl https://github.com/bensadeghi/DecisionTree.jl\n",
    "- ScikitLearn.jl https://github.com/cstjean/ScikitLearn.jl\n",
    "\n",
    "### References \n",
    "\n",
    "[1] The Elements of Statistical Learning (Ch 15.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
